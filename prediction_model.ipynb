{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "164e60dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bff52b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/06 15:37:33 WARN Utils: Your hostname, SweetCard resolves to a loopback address: 127.0.1.1; using 192.168.0.249 instead (on interface wlp2s0)\n",
      "22/04/06 15:37:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/06 15:37:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/04/06 15:37:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkSes\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ffd7e2",
   "metadata": {},
   "source": [
    "Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc5389d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+----------------------+---------------------+---------------------+\n",
      "| user_id|month_interaction_count|week_interaction_count|day_interaction_count|cancelled_within_week|\n",
      "+--------+-----------------------+----------------------+---------------------+---------------------+\n",
      "|66860ae6|                     41|                     9|                    0|                    1|\n",
      "|249803f8|                     25|                     9|                    2|                    0|\n",
      "|32ed74cc|                     21|                     2|                    1|                    1|\n",
      "|7ed76e6a|                     22|                     5|                    2|                    0|\n",
      "|46c81f43|                     32|                     8|                    2|                    0|\n",
      "|cf0f185e|                     26|                     4|                    0|                    1|\n",
      "|568275b3|                     29|                     5|                    1|                    1|\n",
      "|86a060ec|                     33|                     7|                    1|                    1|\n",
      "|c0c07290|                     35|                    10|                    0|                    0|\n",
      "|709dc1da|                     36|                    11|                    1|                    0|\n",
      "+--------+-----------------------+----------------------+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_in = spark.read.csv('dataframe.csv', header=True, inferSchema=True)\n",
    "df_in.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437120c0",
   "metadata": {},
   "source": [
    "Vectorizing data and preparing Train and Test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5416893",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"month_interaction_count\", \n",
    "                                       \"week_interaction_count\", \n",
    "                                       \"day_interaction_count\"],\n",
    "                            outputCol=\"features\")\n",
    "df = assembler.transform(df_in)\n",
    "df_train = df.select(\"user_id\", \"cancelled_within_week\", \"features\")\n",
    "df_test = df.select(\"user_id\", \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4be6b",
   "metadata": {},
   "source": [
    "Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6289b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrgen = LogisticRegression(labelCol=\"cancelled_within_week\", \n",
    "                           featuresCol=\"features\", \n",
    "                           maxIter=10, \n",
    "                           regParam=0.1,\n",
    "                           threshold=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8947d56",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc1a098d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/06 15:40:35 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/04/06 15:40:35 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    }
   ],
   "source": [
    "linearModelgen = lrgen.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46322334",
   "metadata": {},
   "source": [
    "Making prediction on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39e3664c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+----------+\n",
      "| user_id|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+----------+\n",
      "|66860ae6|[-0.6913927322203...|[0.33372332467871...|       1.0|\n",
      "|249803f8|[1.99534219815040...|[0.88030717060971...|       0.0|\n",
      "|32ed74cc|[-1.5519845889443...|[0.17479981666924...|       1.0|\n",
      "|7ed76e6a|[0.55912506936656...|[0.63625007405661...|       0.0|\n",
      "|46c81f43|[1.28102716568289...|[0.78262457201960...|       0.0|\n",
      "|cf0f185e|[-1.9709630540962...|[0.12228548294156...|       1.0|\n",
      "|568275b3|[-0.7384022990095...|[0.32335361572998...|       1.0|\n",
      "|86a060ec|[-0.1348939766407...|[0.46632755009981...|       0.0|\n",
      "|c0c07290|[-0.0229177965620...|[0.49427080161733...|       0.0|\n",
      "|709dc1da|[1.30132315214311...|[0.78605758316099...|       0.0|\n",
      "+--------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_res = linearModelgen.transform(df_test)\n",
    "predictions_out = predictions_res.select('user_id', \n",
    "                                         'rawPrediction', \n",
    "                                         'probability', \n",
    "                                         'prediction')\n",
    "predictions_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281d566",
   "metadata": {},
   "source": [
    "Performing data output into `file predictions.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e79e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_out.toPandas().to_csv('predictions.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b54c7",
   "metadata": {},
   "source": [
    "### Note\n",
    "The case above is a very simplified approach to buliding an ML model.\n",
    "\n",
    "In a more realisitic case we should split the data into Train and Test parts and do some feature engineering and model hyperparameters tuning and pick the best features and hyperparameters using crossvalidation on a Train set. After that we should train the model on the whole Train set and once again check it's performance fitting the model to the Test set to make sure that there's no overfitting to the Train data. On the final step we may combine Train and Test sets and train our model on that combined dataset and use it on the new unknown data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
